# Text-Generation-using-RNN-LSTM
Developed a Text Generation model using RNN/LSTM to generate human-like text sequences. Implemented tokenization, n-gram sequence creation, padding, and embedding layers for training a word-level language model. The model predicts the next word iteratively to generate coherent sentences from a seed input.
